{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0fa1d3d",
   "metadata": {},
   "source": [
    "## This uses transfer learning on a pre-trained classifier to build an Alpaca Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f0a30",
   "metadata": {},
   "source": [
    " It uses MobileNetV2 which has been pre-trained on ImageNet, a dataset containing over 14 million images and 1000 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d187e",
   "metadata": {},
   "source": [
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c593c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2508e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 327 files belonging to 2 classes.\n",
      "Using 262 files for training.\n",
      "Found 327 files belonging to 2 classes.\n",
      "Using 65 files for validation.\n"
     ]
    }
   ],
   "source": [
    "#Generate dataset from images in the dataset folder\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (160, 160)\n",
    "directory = \"dataset/\"\n",
    "# Create training and validation set. Use the same seed to avoid image overlap\n",
    "train_dataset = image_dataset_from_directory(directory,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=IMG_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             subset='training',\n",
    "                                             seed=42)\n",
    "validation_dataset = image_dataset_from_directory(directory,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=IMG_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             subset='validation',\n",
    "                                             seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a718925d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alpaca', 'not alpaca']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view class names\n",
    "class_names = train_dataset.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332c197",
   "metadata": {},
   "source": [
    "## Pre-fetch data and Pre-process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4935079",
   "metadata": {},
   "source": [
    "Using prefetch() prevents a memory bottleneck that can occur when reading from disk. It sets aside some data and keeps it ready for when it's needed, by creating a source dataset from the input data, applying a transformation to preprocess it, then iterating over the dataset one element at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c55673",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE) #choose number of elements to pre-fetch automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97abb4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data will need to be augmented to increase size of dataset\n",
    "\n",
    "def data_augmenter():\n",
    "    '''\n",
    "    Create a Sequential model composed of 2 layers.\n",
    "    layer 1 - causes flip on horinzotal axis\n",
    "    layer 2 -causes rotation\n",
    "    Returns:\n",
    "        tf.keras.Sequential\n",
    "    '''\n",
    "    \n",
    "    data_augmentation = tf.keras.models.Sequential();\n",
    "    data_augmentation.add(RandomFlip('horizontal'))\n",
    "    data_augmentation.add(RandomRotation(0.2))\n",
    "   \n",
    "    \n",
    "    return data_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c00af1",
   "metadata": {},
   "source": [
    "When using a pretrained model, it's best to reuse the weights it was trained on. MobileNetV2 is already part of Keras and we can access it. Since we're using a pre-trained model that was trained on the normalization values [-1,1], it's best practice to reuse that standard with tf.keras.applications.mobilenet_v2.preprocess_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc0038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "#The preprocess_input function is meant to adequate the image to the format the model requires.\n",
    "#Some models use images with values ranging from 0 to 1. Others from -1 to +1\n",
    "# We use the values that were used for the original mobilenetv2. \n",
    "#we can get them from the applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea6582",
   "metadata": {},
   "source": [
    "## Base Model to Alpaca model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb3a7ed",
   "metadata": {},
   "source": [
    "Given the MobileNetV2 base model, the top layers are used for classification. Inorder to convert it to an alpaca classifier, we would exclude these top layers from our base model and then build our model from there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124bd64",
   "metadata": {},
   "source": [
    "Training will be done as follows:\n",
    "- Delete the top layer (the classification layer)\n",
    "- Set include_top in base_model as False\n",
    "- Add a new classifier layer\n",
    "- Train only one layer by freezing the rest of the network\n",
    "- A single neuron is enough to solve a binary classification problem.\n",
    "- Freeze the base model and train the newly-created classifier layer\n",
    "- Set base model.trainable=False to avoid changing the weights and train only the new layer\n",
    "- Set training in base_model to False to avoid keeping track of statistics in the batch norm l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e3b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpaca_model(image_shape=IMG_SIZE, data_augmentation=data_augmenter()):\n",
    "    ''' Define a tf.keras model for binary classification out of the MobileNetV2 model\n",
    "    Arguments:\n",
    "        image_shape -- Image width and height\n",
    "        data_augmentation -- data augmentation function\n",
    "    Returns:\n",
    "    Returns:\n",
    "        tf.keras.model\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    input_shape = image_shape + (3,) # image shape plus 3 dimensions. In this case 160,160,3 (For the colour channels).\n",
    "    \n",
    "   \n",
    "    #This is the base model of the mobileNetV2.Notice how the top layer is excluded\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n",
    "                                                   include_top=False, # <== Important!!!!\n",
    "                                                   weights='imagenet') # From imageNet\n",
    "    \n",
    "    # freeze the base model by making it non trainable\n",
    "    base_model.trainable = False \n",
    "\n",
    "    # create the input layer (Same as the imageNetv2 input size)\n",
    "    inputs = tf.keras.Input(shape=input_shape) \n",
    "    \n",
    "    # apply data augmentation to the inputs\n",
    "    x = data_augmentation(inputs)\n",
    "    \n",
    "    # data preprocessing using the same weights the model was trained on\n",
    "    \n",
    "    x = preprocess_input(x) \n",
    "    \n",
    "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
    "    x = base_model(x, training=False) \n",
    "    \n",
    "    # -----------------add the new Binary classification layers------------------------------\n",
    "    # use global avg pooling to summarize the info in each channel\n",
    "    #Global Average Pooling is a pooling operation designed to replace fully connected layers in classical CNNs. \n",
    "    #The idea is to generate one feature map for each corresponding category of the classification task in the last\n",
    "    #mlpconv layer. \n",
    "    #Instead of adding fully connected layers on top of the feature maps, \n",
    "    #we take the average of each feature map, and the resulting vector is fed directly into the softmax layer.\n",
    "    \n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x) \n",
    "    \n",
    "    # include dropout with probability of 0.2 to avoid overfitting\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "    # use a prediction layer with one neuron (as a binary classifier only needs one)\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "    \n",
    "   ###Â END CODE HERE\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
